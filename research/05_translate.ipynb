{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'/media/puzan/NewVolume/MyFolder/research'"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os\n",
    "os.getcwd()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "os.chdir(\"../\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "from dataclasses import dataclass\n",
    "from pathlib import Path\n",
    "\n",
    "@dataclass(frozen=True)\n",
    "class ModelTranslateConfig:\n",
    "    root_dir:Path\n",
    "    tokenizer_file:Path\n",
    "    src_lang:str\n",
    "    tgt_lang:Path\n",
    "    max_seq_len:int\n",
    "    model_path:Path\n",
    "    model_basename:Path\n",
    "    epoch_name: int\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from translator.constants import *\n",
    "from translator.utils.common import read_yaml,create_directories"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ConfigurationManager:\n",
    "    def __init__(self,config_file_path=CONFIG_FILE_PATH,params_file_path=PARAMS_FILE_PATH):\n",
    "        self.config=read_yaml(config_file_path)\n",
    "        self.params=read_yaml(params_file_path)\n",
    "        create_directories([self.config.artifacts_root])\n",
    "    \n",
    "    def get_model_translate_config(self)-> ModelTranslateConfig:\n",
    "        config=self.config.data_translate\n",
    "        params=self.params.modeltrainer\n",
    "        print(len(params))\n",
    "        create_directories([config.root_dir])\n",
    "        model_translate_config=ModelTranslateConfig(\n",
    "            root_dir=config.root_dir,\n",
    "            tokenizer_file=config.tokenizer_file,\n",
    "            src_lang=params.src_lang,\n",
    "            tgt_lang=params.tgt_lang,\n",
    "            max_seq_len=params.max_seq_len,\n",
    "            model_path=config.model_path,\n",
    "            model_basename=config.model_basename,\n",
    "            epoch_name=config.epoch_name\n",
    "\n",
    "        )\n",
    "        return model_translate_config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tokenizers import Tokenizer\n",
    "from translator.model import build_transformer\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Load_model:\n",
    "    def __init__(self,config:ModelTranslateConfig):\n",
    "        self.config=config\n",
    "        self.model=None\n",
    "        self.tokenizer_src=None\n",
    "        self.tokenizer_tgt=None\n",
    "        self.device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "    def get_weights_file_path(self,epoch:str):\n",
    "        model_folder=self.config.model_path # extracting model folder from the config\n",
    "        model_basename=self.config.model_basename# extracting the base name for model files\n",
    "        model_filename=f'{model_basename}{epoch}.pt'\n",
    "        return str(Path('.')/model_folder/model_filename)\n",
    "    \n",
    "    def load_model(self):\n",
    "        \n",
    "        self.tokenizer_src = Tokenizer.from_file(self.config.tokenizer_file.format(self.config.src_lang))\n",
    "        self.tokenizer_tgt = Tokenizer.from_file(self.config.tokenizer_file.format(self.config.tgt_lang))\n",
    "        vocab_src_len = self.tokenizer_src.get_vocab_size()\n",
    "        vocab_tgt_len = self.tokenizer_tgt.get_vocab_size()\n",
    "        self.model = build_transformer(vocab_src_len, vocab_tgt_len, self.config.max_seq_len, self.config.max_seq_len)\n",
    "        self.model = self.model.to(self.device)\n",
    "        \n",
    "        model_filename = self.get_weights_file_path(f\"epoch_{self.config.epoch_name}\")\n",
    "        state = torch.load(model_filename, map_location=self.device)\n",
    "        self.model.load_state_dict(state['model_state_dict'])\n",
    "        \n",
    "    def preprocess_sentence(self,sentence, tokenizer, seq_len, sos_token, eos_token, pad_token):\n",
    "        tokens = tokenizer.encode(sentence).ids\n",
    "        num_padding_tokens = seq_len - len(tokens) - 2\n",
    "\n",
    "        if num_padding_tokens < 0:\n",
    "            raise ValueError(\"Sentence is too long\")\n",
    "\n",
    "        input_tensor = torch.cat([\n",
    "            torch.tensor([sos_token], dtype=torch.int64),\n",
    "            torch.tensor(tokens, dtype=torch.int64),\n",
    "            torch.tensor([eos_token], dtype=torch.int64),\n",
    "            torch.tensor([pad_token] * num_padding_tokens, dtype=torch.int64)\n",
    "        ])\n",
    "\n",
    "        return input_tensor.unsqueeze(0)\n",
    "\n",
    "    def translate_sentence(self,sentence):\n",
    "        sos_token = self.tokenizer_tgt.token_to_id('[SOS]')\n",
    "        eos_token = self.tokenizer_tgt.token_to_id('[EOS]')\n",
    "        pad_token = self.tokenizer_tgt.token_to_id('[PAD]')\n",
    "\n",
    "        encoder_input = self.preprocess_sentence(sentence, self.tokenizer_src, self.config.max_seq_len, sos_token, eos_token, pad_token).to(self.device)\n",
    "        encoder_mask = (encoder_input != pad_token).unsqueeze(1).unsqueeze(2).int().to(self.device)\n",
    "\n",
    "        encoder_output = self.model.encode(encoder_input, encoder_mask)\n",
    "        decoder_input = torch.tensor([[sos_token]], dtype=torch.int64).to(self.device)\n",
    "\n",
    "        for _ in range(self.config.max_seq_len):\n",
    "            decoder_mask = torch.triu(torch.ones((1, decoder_input.size(1), decoder_input.size(1))), diagonal=1).type_as(encoder_mask).to(self.device)\n",
    "            decoder_output = self.model.decode(encoder_output, encoder_mask, decoder_input, decoder_mask)\n",
    "            proj_output = self.model.project(decoder_output[:, -1])\n",
    "\n",
    "            _, next_word = torch.max(proj_output, dim=1)\n",
    "            decoder_input = torch.cat([decoder_input, next_word.unsqueeze(0)], dim=1)\n",
    "\n",
    "            if next_word.item() == eos_token:\n",
    "                break\n",
    "\n",
    "        translated_tokens = decoder_input.squeeze(0).tolist()\n",
    "        translated_text = self.tokenizer_tgt.decode(translated_tokens)\n",
    "        return translated_text\n",
    "\n",
    "\n",
    "        \n",
    "\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2024-09-26 16:45:52,154:INFO:common:yaml_file:config/config.yaml loaded successfully\n",
      "[2024-09-26 16:45:52,157:INFO:common:yaml_file:param/params.yaml loaded successfully\n",
      "[2024-09-26 16:45:52,158:INFO:common:Directory 'artifacts' created successfully\n",
      "7\n",
      "[2024-09-26 16:45:52,159:INFO:common:Directory 'artifacts/data_translate' created successfully\n",
      "torch.Size([1, 150, 512])\n",
      "torch.Size([1, 150, 512])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_32504/162301265.py:24: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  state = torch.load(model_filename, map_location=self.device)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "рдкрдирд┐\n"
     ]
    }
   ],
   "source": [
    "try:\n",
    "    config=ConfigurationManager()\n",
    "    data_translate_config=config.get_model_translate_config()\n",
    "    mod=Load_model(data_translate_config)\n",
    "    mod.load_model()\n",
    "    translated_sentence = mod.translate_sentence(\"This is a test sentence.\")\n",
    "    print(translated_sentence)\n",
    "except Exception as e:\n",
    "    raise e\n",
    "    \n",
    "    model_loader = Load_model(config)\n",
    "\n",
    "# Load the model and tokenizers (only once\n",
    "\n",
    "# Translate a sentence (no need to pass model or tokenizers again)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
