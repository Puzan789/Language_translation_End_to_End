{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'/media/puzan/NewVolume/MyFolder/research'"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "os.getcwd()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "os.chdir(\"../\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'/media/puzan/NewVolume/MyFolder'"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "os.getcwd()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "from dataclasses import dataclass\n",
    "from pathlib import Path\n",
    "@dataclass(frozen=True)\n",
    "class DataTransformConfig:\n",
    "    root_dir:Path\n",
    "    data_path:Path\n",
    "    tokenizer_file:Path\n",
    "    max_seq_len:int\n",
    "    src_lang:str\n",
    "    tgt_lang:str\n",
    "    src_file:Path\n",
    "    tgt_file:Path\n",
    "    batch_size:int\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from translator.constants import *\n",
    "from translator.utils.common import read_yaml,create_directories"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "#configuration manager\n",
    "class ConfigurationManager:\n",
    "    def __init__(self,config_file_path=CONFIG_FILE_PATH,params_file_path=PARAMS_FILE_PATH):\n",
    "        self.config=read_yaml(config_file_path)\n",
    "        self.params=read_yaml(params_file_path)\n",
    "        create_directories([self.config.artifacts_root])\n",
    "    \n",
    "    def get_data_transformation_config(self)-> DataTransformConfig:\n",
    "        config=self.config.data_transformation\n",
    "        params=self.params.modeltrainer\n",
    "        print(len(params))\n",
    "        print(\"running\")\n",
    "        create_directories([config.root_dir])\n",
    "        data_transformation_config=DataTransformConfig(\n",
    "            root_dir=config.root_dir,\n",
    "            data_path=config.data_path,\n",
    "            tokenizer_file=config.tokenizer_file,\n",
    "            max_seq_len=params.max_seq_len,\n",
    "            src_lang=params.src_lang,\n",
    "            tgt_lang=params.tgt_lang,\n",
    "            src_file=config.src_file,\n",
    "            tgt_file=config.tgt_file,\n",
    "            batch_size=params.batch_size\n",
    "        )\n",
    "        return data_transformation_config\n",
    "\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tokenizers import Tokenizer\n",
    "from tokenizers.models import WordLevel\n",
    "from tokenizers.trainers import WordLevelTrainer\n",
    "from tokenizers.pre_tokenizers import Whitespace\n",
    "from datasets import load_dataset\n",
    "from torch.utils.data import Dataset,random_split,DataLoader\n",
    "import torch\n",
    "from typing import Any\n",
    "from translator.logging import logger\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DataTransformation:\n",
    "    def __init__(self,config:DataTransformConfig):\n",
    "        self.config=config\n",
    "        \n",
    "    \n",
    "    def get_all_sentences(self,ds,lang):\n",
    "        for pair in ds:\n",
    "            yield pair['translation'][lang]\n",
    "    \n",
    "    def build_tokenizer(self,ds,lang):\n",
    "        tokenizer_path=Path(str(self.config.tokenizer_file).format(lang))\n",
    "        if not Path.exists(tokenizer_path):\n",
    "            tokenizer=Tokenizer(WordLevel(unk_token='[UNK]'))\n",
    "            tokenizer.pre_tokenizer=Whitespace() # we will spilt the text into tokens based ont hte whitespace\n",
    "\n",
    "            # creating a trainer for the new tokenizer \n",
    "            trainer=WordLevelTrainer(special_tokens=['[UNK]','[PAD]','[SOS]','[EOS]'])\n",
    "            tokenizer.train_from_iterator(self.get_all_sentences(ds,lang),trainer=trainer)\n",
    "            tokenizer.save(str(tokenizer_path))\n",
    "            logger.info(f\"Tokenizer saved\")\n",
    "        else:\n",
    "            tokenizer=Tokenizer.from_file(str(tokenizer_path))\n",
    "            logger.info(\"Tokenizer retrieved from file\")\n",
    "        return tokenizer\n",
    "\n",
    "class BilingualDataset(Dataset):\n",
    "    def __init__(self,ds,tokenizer_src,tokenizer_tgt,config:DataTransformConfig,) :\n",
    "        super().__init__()\n",
    "        self.config=config\n",
    "        self.seq_len=self.config.max_seq_len\n",
    "        self.ds=ds\n",
    "        self.tokenizer_src=tokenizer_src\n",
    "        self.tokenizer_tgt=tokenizer_tgt\n",
    "        self.src_lang=self.config.src_lang\n",
    "        self.tgt_lang=self.config.tgt_lang\n",
    "        self.sos_token=torch.tensor([tokenizer_tgt.token_to_id('[SOS]')],dtype=torch.int64)\n",
    "        self.eos_token=torch.tensor([tokenizer_tgt.token_to_id('[EOS]')], dtype=torch.int64)\n",
    "        self.pad_token=torch.tensor([tokenizer_tgt.token_to_id('[PAD]')], dtype=torch.int64)\n",
    "    def __len__(self):\n",
    "        return len(self.ds)\n",
    "    def casual_mask(self,size):\n",
    "        mask=torch.triu(torch.ones(1,size,size),diagonal=1).type(torch.int)\n",
    "        return mask==0\n",
    "    \n",
    "    def __getitem__(self,index:Any):\n",
    "        src_target_pair=self.ds[index]\n",
    "        src_text=src_target_pair['translation'][self.src_lang]\n",
    "        tgt_text=src_target_pair['translation'][self.tgt_lang]\n",
    "\n",
    "        #tokenizationgthe source and target text \n",
    "        enc_input_tokens=self.tokenizer_src.encode(src_text).ids\n",
    "        dec_input_tokens=self.tokenizer_tgt.encode(tgt_text).ids\n",
    "\n",
    "        # sentence ma hamlai aktiota pad token chainxa \n",
    "        enc_num_padding_tokens=self.seq_len-len(enc_input_tokens) -2 # -2 for eos and sos\n",
    "\n",
    "        #target tokens \n",
    "        dec_num_padding_tokens=self.seq_len-len(dec_input_tokens)-1 # euta chai for sos\n",
    "\n",
    "        if enc_num_padding_tokens<0 or dec_num_padding_tokens<0:\n",
    "            logger.error(\"Sentences are long\")\n",
    "            raise ValueError(\"Sentences seem to be long\")#yedi maxtokens 10 xa aani tokens 9 ota xa bhaney eos ra sos nai bhayena jaha -1 aauxa tei bahyera\n",
    "        \n",
    "        #suruma sos tokens last ma eos token ani padding tokens\n",
    "        encoder_input=torch.cat(\n",
    "            [\n",
    "                self.sos_token,\n",
    "                torch.tensor(enc_input_tokens,dtype=torch.int64),\n",
    "                self.eos_token,\n",
    "                torch.tensor([self.pad_token]*enc_num_padding_tokens,dtype=torch.int64)#padding tokens add gareko jastai list[0]*5 huda list[0,0,0,0,0]\n",
    "\n",
    "            ]\n",
    "        )\n",
    "\n",
    "        #building decoder input tensor\n",
    "        decoder_input=torch.cat([\n",
    "            self.sos_token, # inserting the '[SOS]' token\n",
    "            torch.tensor(dec_input_tokens, dtype=torch.int64), # indersting the tokenized target text\n",
    "            torch.tensor([self.pad_token] * dec_num_padding_tokens, dtype=torch.int64) # adding padding tokens\n",
    "        ])\n",
    "\n",
    "        # yo bhaneko label target yo sanga comaper garera loss nikalxa\n",
    "        # creating a label tensor, the expected output for training the model\n",
    "        label=torch.cat([\n",
    "            torch.tensor(dec_input_tokens, dtype=torch.int64), # inserting the tokenized targate text\n",
    "            self.eos_token, # inserting the '[EOS]' token\n",
    "            torch.tensor([self.pad_token] * dec_num_padding_tokens, dtype=torch.int64) # adding padding tokens\n",
    "        ])\n",
    "        # Ensuring that the length of each tensor above is equal to the defined `seq_len`\n",
    "        assert encoder_input.size(0)==self.seq_len,'Encoder input doesnt match with sequencelength'\n",
    "        assert decoder_input.size(0)==self.seq_len,'Edecoder input doesnt match with sequencelength'\n",
    "        assert label.size(0)==self.seq_len,'label  doesnt match with sequencelength'\n",
    "\n",
    "        return {\n",
    "            'encoder_input':encoder_input,\n",
    "            'decoder_input':decoder_input,\n",
    "            'encoder_mask': (encoder_input!=self.pad_token).unsqueeze(0).unsqueeze(0).int(),\n",
    "            'decoder_mask': (decoder_input!=self.pad_token).unsqueeze(0).unsqueeze(0).int() & self.casual_mask(decoder_input.size(0)),\n",
    "            'label':label,\n",
    "            'src_text': src_text,\n",
    "            'tgt_text': tgt_text\n",
    "        }\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "class GetDataset:\n",
    "    def __init__(self,config:DataTransformConfig):\n",
    "        self.config=config\n",
    "        self.data_transformer=DataTransformation(config)\n",
    "\n",
    "    def read_text_files(self):\n",
    "        with open(self.config.src_file,'r',encoding='utf-8') as src_f ,open(self.config.tgt_file,\"r\",encoding='utf-8') as tgt_f :\n",
    "            src_lines=src_f.readlines()\n",
    "            tgt_lines=tgt_f.readlines()\n",
    "        \n",
    "        assert len(src_lines) ==len(tgt_lines) ,\"Source and target files must have the same number of lines and lengths\"\n",
    "        dataset=[{'translation':{'src':src.strip(),'tgt':tgt.strip()}} for src,tgt in zip(src_lines,tgt_lines)]\n",
    "        return dataset\n",
    "    def get_ds(self):\n",
    "        #read dataset form text file\n",
    "        ds_raw=self.read_text_files()\n",
    "        # building and loading tokenizer for source and target file\n",
    "        tokenizer_src=self.data_transformer.build_tokenizer(ds_raw,self.config.src_lang)\n",
    "        tokenizer_tgt=self.data_transformer.build_tokenizer(ds_raw,self.config.tgt_lang)\n",
    "\n",
    "        #splitting the dataset for training and validation\n",
    "        train_ds_size=int(0.9 * len (ds_raw))\n",
    "        val_ds_size=len(ds_raw) -train_ds_size\n",
    "        train_ds_raw,val_ds_raw=random_split(ds_raw,[train_ds_size,val_ds_size])\n",
    "\n",
    "        #processing dataset with bilingualdataset \n",
    "        train_ds=BilingualDataset(ds=train_ds_raw,tokenizer_src=tokenizer_src,tokenizer_tgt=tokenizer_tgt,config=self.config)\n",
    "        val_ds=BilingualDataset(ds=val_ds_raw,tokenizer_src=tokenizer_src,tokenizer_tgt=tokenizer_tgt,config=self.config)\n",
    "\n",
    "        #finding the maximum length in the dataset \n",
    "        max_len_src=0\n",
    "        max_len_tgt=0\n",
    "        for pair in ds_raw:\n",
    "            src_ids=tokenizer_src.encode(pair['translation'][self.config.src_lang]).ids\n",
    "            tgt_ids=tokenizer_tgt.encode(pair['translation'][self.config.tgt_lang]).ids\n",
    "\n",
    "            max_len_src=max(max_len_src,len(src_ids))\n",
    "            max_len_tgt=max(max_len_tgt,len(tgt_ids))\n",
    "\n",
    "        print(f\"Max Length of source Sentence: {max_len_src}\")\n",
    "        print(f\"Max Length of target Sentence: {max_len_tgt}\")\n",
    "        logger.info(f\"The  max length of source sentence is {max_len_src}\")\n",
    "        logger.info(f\"The  max length of target sentence is {max_len_tgt}\")\n",
    "\n",
    "\n",
    "        train_dataloader=DataLoader(train_ds,batch_size=self.config.batch_size,shuffle=True)\n",
    "        val_dataloader=DataLoader(val_ds,batch_size=1,shuffle=True)\n",
    "\n",
    "        return train_dataloader,val_dataloader,tokenizer_src,tokenizer_tgt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2024-09-25 21:53:11,763:INFO:common:yaml_file:config/config.yaml loaded successfully\n",
      "[2024-09-25 21:53:11,767:INFO:common:yaml_file:param/params.yaml loaded successfully\n",
      "[2024-09-25 21:53:11,771:INFO:common:Directory 'artifacts' created successfully\n",
      "4\n",
      "running\n",
      "[2024-09-25 21:53:11,773:INFO:common:Directory 'artifacts/data_transformation' created successfully\n",
      "[2024-09-25 21:53:11,777:INFO:2351436263:Tokenizer retrieved from file\n",
      "[2024-09-25 21:53:11,779:INFO:2351436263:Tokenizer retrieved from file\n",
      "Max Length of source Sentence: 14\n",
      "Max Length of target Sentence: 11\n",
      "[2024-09-25 21:53:11,788:INFO:449058478:The  max length of source sentence is 14\n",
      "[2024-09-25 21:53:11,790:INFO:449058478:The  max length of target sentence is 11\n"
     ]
    }
   ],
   "source": [
    "try:\n",
    "    config=ConfigurationManager()\n",
    "    data_transformation_config=config.get_data_transformation_config()\n",
    "    get_dataset=GetDataset(data_transformation_config)\n",
    "    success=get_dataset.get_ds()\n",
    "except Exception as e :\n",
    "    raise e"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(<torch.utils.data.dataloader.DataLoader at 0x7f70e9b2be00>,\n",
       " <torch.utils.data.dataloader.DataLoader at 0x7f70fa8729c0>,\n",
       " <tokenizers.Tokenizer at 0x7f70d45b6e30>,\n",
       " <tokenizers.Tokenizer at 0x7f70ab91d630>)"
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
