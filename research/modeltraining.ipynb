{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import math\n",
    "from typing import Any\n",
    "if torch.cuda.is_available():\n",
    "    torch.backends.cudnn.deterministic=True\n",
    "from translator.logging import logger"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "from translator.constants import *\n",
    "from translator.utils.common import read_yaml,create_directories"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True\n"
     ]
    }
   ],
   "source": [
    "print(torch.cuda.is_available())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "from dataclasses import dataclass\n",
    "from pathlib import Path\n",
    "\n",
    "@dataclass(frozen=True)\n",
    "class ModelTrainingConfig:\n",
    "    model_path:Path\n",
    "    experiment_path:Path\n",
    "    max_seq_len:int\n",
    "    d_model:int\n",
    "    lr:float\n",
    "    preload:str|None=None\n",
    "    model_basename:str\n",
    "    num_epochs:int\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ConfigurationManager:\n",
    "    def __init__(self,config_file_path=CONFIG_FILE_PATH,params_file_path=PARAMS_FILE_PATH):\n",
    "        self.config=read_yaml(config_file_path)\n",
    "        self.params=read_yaml(params_file_path)\n",
    "        create_directories([self.config.artifacts_root])\n",
    "    \n",
    "    def get_model_training_config(self)-> ModelTrainingConfig:\n",
    "        config=self.config.data_training\n",
    "        params=self.params.modeltrainer\n",
    "        print(len(params))\n",
    "        print(\"running\")\n",
    "        create_directories([config.root_dir])\n",
    "        model_training_config=ModelTrainingConfig(\n",
    "            root_dir=config.root_dir,\n",
    "            model_path=config.model_path,\n",
    "            experiment_path=config.experimnet_path,\n",
    "            max_seq_len=params.max_seq_len,\n",
    "            d_model=params.d_model,\n",
    "            lr=params.learning_rate,\n",
    "            preload=config.preload,\n",
    "            model_basename=config.model_basename,\n",
    "            num_epochs=params.num_epochs\n",
    "\n",
    "        )\n",
    "        return model_training_config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Model \n",
    "# embeddings\n",
    "\n",
    "class Inputembedding(nn.Module):\n",
    "    def __init__(self,d_model:int,vocab_size:int):\n",
    "        super().__init__()\n",
    "        self.d_model=d_model\n",
    "        self.vocab_size=vocab_size\n",
    "        self.embedding=nn.Embedding(vocab_size,d_model)\n",
    "    def forward(self,x):\n",
    "        return self.embedding(x)*math.sqrt(self.d_model)\n",
    "\n",
    "class PositionalEncoding(nn.Module):\n",
    "    def __init__(self,d_model:int,max_seq_len:int,dropout:float):\n",
    "        super().__init__()\n",
    "        self.d_model=d_model\n",
    "        self.max_seq_len=max_seq_len\n",
    "        self.dropout=nn.Dropout(dropout)\n",
    "        #positional encoding filled with  zeroes\n",
    "        pe=torch.zeros(max_seq_len,d_model)\n",
    "        # creating a position\n",
    "        position=torch.arange(0,max_seq_len,dtype=torch.float).unsqueeze(1)\n",
    "        dividend_term=torch.exp(torch.arange(0,d_model,2).float()*(-math.log(10000)/d_model))\n",
    "        #applying sine to even indices\n",
    "        pe[:,0::2]=torch.sin(position*dividend_term)\n",
    "        #applying cosine to odd indices\n",
    "        pe[:,1::2]=torch.cos(position*dividend_term)\n",
    "        #apply one dimension more for the batch_size\n",
    "        pe=pe.unsqueeze(0)\n",
    "        self.register_buffer(\"pe\",pe)\n",
    "        print(pe.shape)\n",
    "    def forward(self,x):\n",
    "        x=x+(self.pe[:,:x.shape[1],:]) # all bach size 0 to maxseqlen-1,dimension \n",
    "        return self.dropout(x)\n",
    "class LayerNormalization(nn.Module):\n",
    "    def __init__(self,eps:float=10**-6):\n",
    "        super().__init__()\n",
    "        self.eps=eps\n",
    "        self.alpha=nn.Parameter(torch.ones(1))\n",
    "        self.bias=nn.Parameter(torch.zeros(1))\n",
    "    \n",
    "    def forward(self,x):\n",
    "        mean=x.mean(dim=-1,keepdim=True)\n",
    "        std=x.std(dim=-1,keepdim=True)\n",
    "        return self.alpha*(x-mean)/(std+self.eps)+self.bias\n",
    "\n",
    "class FeedForwardNeuralNetwork(nn.Module):\n",
    "    def __init__(self,d_model:int,d_ff:int,dropout:float):\n",
    "        super().__init__()\n",
    "        self.firstlayer=nn.Linear(d_model,d_ff)\n",
    "        self.dropout=nn.Dropout(dropout)\n",
    "        self.secondlayer=nn.Linear(d_ff,d_model)\n",
    "    \n",
    "    def forward(self,x):\n",
    "        return self.secondlayer(self.dropout(torch.relu(self.firstlayer(x))))\n",
    "\n",
    "class MultiHeadAttention(nn.Module):\n",
    "    def __init__(self,d_model:int,num_heads:int,dropout:float):\n",
    "        super().__init__()\n",
    "        self.d_model=d_model\n",
    "        self.num_heads=num_heads\n",
    "        assert d_model%num_heads==0,'Dimension of model should be divisible by no of heads'\n",
    "        self.d_k=d_model//num_heads\n",
    "\n",
    "        # for the  weight metrices\n",
    "        self.w_q=nn.Linear(d_model,d_model)# Weighted query\n",
    "        self.w_k=nn.Linear(d_model,d_model)# weighted key\n",
    "        self.w_v=nn.Linear(d_model,d_model)# weighted value\n",
    "        self.w_o=nn.Linear(d_model,d_model) #weight of the concatenated layer\n",
    "        self.dropout=nn.Dropout(dropout)  #last dropoutlayer\n",
    "    @staticmethod\n",
    "    def attention(query,key,value,mask,dropout=nn.Dropout):\n",
    "        d_k=query.shape[-1]\n",
    "        attention_scores=(query@key.transpose(-2,-1))/math.sqrt(d_k)\n",
    "        if mask is not None:\n",
    "            attention_scores.masked_fill_(mask==0,-1e9)# In-place: mask out positions with a large negative value to ignore them in softmax\n",
    "        attention_scores=attention_scores.softmax(dim=-1) # applied at the last dimension that is max_selenght\n",
    "        if dropout is not None:\n",
    "            attention_scores=dropout(attention_scores)\n",
    "        return (attention_scores@value) ,attention_scores\n",
    "\n",
    "    def forward(self,q,k,v,mask):\n",
    "        query=self.w_q(q)\n",
    "        key=self.w_k(k)\n",
    "        value=self.w_v(v)\n",
    "        query=query.view(query.shape[0],query.shape[1],self.num_heads,self.d_k).transpose(1,2) #batchsize sequencelength number of head,d_k #transpose chai aaba independently head lai train garxam so batchsize,oofheads,max_seq_len,d_k hunxa\n",
    "        key=key.view(key.shape[0],key.shape[1],self.num_heads,self.d_k).transpose(1,2)\n",
    "        value=value.view(value.shape[0],value.shape[1],self.num_heads,self.d_k).transpose(1,2)\n",
    "        #obtain output and attention scores\n",
    "        x,self.attention_scores=MultiHeadAttention.attention(query,key,value,mask,self.dropout)\n",
    "        # create  a concatenated matrix\n",
    "        x=x.transpose(1,2).contiguous().view(x.shape[0],-1,self.num_heads*self.d_k)#\n",
    "        return self.w_o(x)\n",
    "    \n",
    "class ResidualConnection(nn.Module):\n",
    "    def __init__(self, dropout: float) -> None:\n",
    "        super().__init__()\n",
    "        # we use a dropout layer to prevent overfitting\n",
    "        self.dropout=nn.Dropout(dropout)\n",
    "        # we use a normalization layer\n",
    "        self.norm=LayerNormalization()\n",
    "        \n",
    "    def forward(self, x, sublayer):\n",
    "        # we normalize the input and add it to the original input x`. This creates the residual connection process\n",
    "        return x+self.dropout(sublayer(self.norm(x)))\n",
    "\n",
    "# Building Encoder Block\n",
    "class EncoderBlock(nn.Module):\n",
    "    def __init__(self,self_attention_block:MultiHeadAttention,ffn:FeedForwardNeuralNetwork,dropout:float):\n",
    "        super().__init__()\n",
    "        self.self_attention_block=self_attention_block\n",
    "        self.ffn=ffn\n",
    "        self.residual_connections=nn.ModuleList([ResidualConnection(dropout) for _ in range(2)])\n",
    "    def forward(self,x,src_mask):\n",
    "        x=self.residual_connections[0](x,lambda x : self.self_attention_block(x,x,x,src_mask)) \n",
    "        x=self.residual_connections[1](x,self.ffn) # x+x.self.ffn(x)\n",
    "        # output tensor with applying feedforward selfattention feedforward \n",
    "        return x\n",
    "\n",
    "class Encoder(nn.Module):\n",
    "    def __init__(self, layers: nn.ModuleList)-> None:\n",
    "        super().__init__()\n",
    "        self.layers=layers # storing the EncoderBlocks\n",
    "        # layer for the normalization of the output of the encoder layers\n",
    "        self.norm=LayerNormalization()\n",
    "    \n",
    "    def forward(self, x, mask):\n",
    "        # Iterating over each EncoderBlock stored in self.layers\n",
    "        for layer in self.layers:\n",
    "            # Applying each EncoderBlock to the input tensor x\n",
    "            x=layer(x, mask)\n",
    "        return self.norm(x) # Normalizing output After running all n layers\n",
    "\n",
    "# Decoderblock #it takes multihead attention and crossattention\n",
    "class DecoderBlock(nn.Module):\n",
    "    def __init__(self,self_attention_block:MultiHeadAttention,crossattentionblock:MultiHeadAttention,ffn:FeedForwardNeuralNetwork,dropout:float):\n",
    "        super().__init__()\n",
    "        self.self_attention_block=self_attention_block\n",
    "        self.cross_attention_block=crossattentionblock\n",
    "        self.ffn=ffn\n",
    "        self.residual_connection=nn.ModuleList([ResidualConnection(dropout) for _ in range(3)])\n",
    "    \n",
    "    def forward(self,x,encoderoutput,src_mask,tgt_mask):\n",
    "        x=self.residual_connection[0](x,lambda x :self.self_attention_block(x,x,x,tgt_mask))\n",
    "        x=self.residual_connection[1](x,lambda x : self.cross_attention_block(x,encoderoutput,encoderoutput,src_mask))\n",
    "        x=self.residual_connection[2](x,self.ffn)\n",
    "        return x\n",
    "\n",
    "\n",
    "class Decoder(nn.Module):\n",
    "    def __init__(self,layers:nn.ModuleList):\n",
    "        super().__init__()\n",
    "        self.layers=layers\n",
    "        self.norm=LayerNormalization()\n",
    "    \n",
    "    def forward(self,x,encoder_output,src_mask,tgt_mask):\n",
    "        for layer in self.layers:\n",
    "            x=layer(x,encoder_output,src_mask,tgt_mask)\n",
    "        return self.norm(x)\n",
    "# Projection layer\n",
    "\n",
    "class ProjectionLayer(nn.Module):\n",
    "    def __init__(self,d_model:int,vocab_size:int):\n",
    "        super().__init__()\n",
    "        self.projection=nn.Linear(d_model,vocab_size)\n",
    "    def forward(self,x):\n",
    "        return torch.log_softmax(self.projection(x),dim=-1)\n",
    "\n",
    "# The Transformer Architecture\n",
    "# Contains all the Encoder Decoder Embeddings\n",
    "class Transformer(nn.Module):\n",
    "    def __init__(self,encoder:Encoder,decoder:Decoder,src_embeding:Inputembedding,tgt_embedding:Inputembedding,src_position:PositionalEncoding,tgt_position:PositionalEncoding,projection_layer:ProjectionLayer) -> None:\n",
    "        super().__init__()\n",
    "        self.encoder=encoder\n",
    "        self.decoder=decoder\n",
    "        self.src_embedding=src_embeding\n",
    "        self.tgt_embedding=tgt_embedding\n",
    "        self.src_position=src_position\n",
    "        self.tgt_position=tgt_position\n",
    "        self.projection_layer=projection_layer\n",
    "    \n",
    "    def encode(self,source,src_mask):\n",
    "        #applying embedding to the input source language\n",
    "        source=self.src_embedding(source)\n",
    "        #applying positionalencoding to the source language\n",
    "        source=self.src_position(source)\n",
    "        return self.encoder(source,src_mask)\n",
    "\n",
    "    def decode(self,encoder_output,src_mask,target,tgt_mask):\n",
    "        target=self.tgt_embedding(target)\n",
    "        target=self.tgt_position(target)\n",
    "        return self.decoder(target,encoder_output,src_mask,tgt_mask)\n",
    "        \n",
    "    #applying projection with softmax\n",
    "    def project(self,x):\n",
    "        return self.projection_layer(x)\n",
    "def build_transformer(src_vocab_size:int,tgt_vocab_size:int,src_seq_len:int,tgt_seq_len:int,d_model:int=512,N:int=6,h:int=8,dropout:float=0.2,d_ff:int=2048) -> Transformer:\n",
    "    # Creating Embedding Layers\n",
    "    src_embed=Inputembedding(d_model,src_vocab_size)\n",
    "    tgt_embed=Inputembedding(d_model,tgt_vocab_size)\n",
    "    #Creating Positional Encoding Layers\n",
    "    src_pos=PositionalEncoding(d_model,src_seq_len,dropout)\n",
    "    tgt_pos=PositionalEncoding(d_model,tgt_seq_len,dropout)\n",
    "\n",
    "    # Encoders Blocks\n",
    "    encoder_blocks=[]\n",
    "    for _ in range (N) :\n",
    "        encoder_self_attention_block=MultiHeadAttention(d_model,h,dropout)\n",
    "        feed_forward_block=FeedForwardNeuralNetwork(d_model,d_ff,dropout)\n",
    "        # one layer encoder block\n",
    "        encoder_block=EncoderBlock(encoder_self_attention_block,feed_forward_block,dropout)\n",
    "        encoder_blocks.append(encoder_block)\n",
    "    \n",
    "    # creating decoder blocks\n",
    "    decoder_blocks=[]\n",
    "    for _ in range(N):\n",
    "        decoder_self_attention_block=MultiHeadAttention(d_model,h,dropout)\n",
    "        decoder_cross_attention_block=MultiHeadAttention(d_model,h,dropout)\n",
    "        feed_forward_block=FeedForwardNeuralNetwork(d_model,d_ff,dropout)\n",
    "\n",
    "        # decoder block\n",
    "        decoder_block=DecoderBlock(decoder_self_attention_block,decoder_cross_attention_block,feed_forward_block,dropout)\n",
    "        decoder_blocks.append(decoder_block)\n",
    "    \n",
    "    # Encoder and decoder \n",
    "    encoder=Encoder(nn.ModuleList(encoder_blocks))\n",
    "    decoder=Decoder(nn.ModuleList(decoder_blocks))\n",
    "\n",
    "    #projection layer\n",
    "    projection_layer=ProjectionLayer(d_model,tgt_vocab_size)\n",
    "    # Fulltransforer\n",
    "    transformer=Transformer(encoder,decoder,src_embed,tgt_embed,src_pos,tgt_pos,projection_layer)\n",
    "    #initializing all the parameters\n",
    "    for p in transformer.parameters():\n",
    "        if p.dim()>1:\n",
    "            nn.init.xavier_uniform_(p)\n",
    "    \n",
    "    return transformer\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "from translator.constants import *\n",
    "from translator.utils.common import read_yaml,create_directories\n",
    "from translator.pipeline.stage_03_datatransformation import DataTransformationPipeline\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class train_model:\n",
    "    def __init__(self,config:ModelTrainingConfig):\n",
    "        self.config=config\n",
    "    \n",
    "    def get_weights_file_path(self,epoch:str):\n",
    "        model_folder=self.config.model_path # extracting model folder from the config\n",
    "        model_basename=self.config.model_basename# extracting the base name for model files\n",
    "        model_filename=f'{model_basename}{epoch}.pt'\n",
    "        return str(Path('.')/model_folder/model_filename)\n",
    "    \n",
    "    def get_model(self,vocab_src_len,vocab_tgt_len):\n",
    "        model=build_transformer(vocab_src_len, vocab_tgt_len, self.config.max_seq_len, self.config.max_seq_len, self.config.d_model)\n",
    "        return model\n",
    "\n",
    "    \n",
    "    def trainmodel(self):\n",
    "        device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "        logger.info(f\"Using device{device}\")\n",
    "        #model directory to store weight\n",
    "        self.config.model_path.mkdir(parents=True,exist_ok=True)\n",
    "        #Experiment directory\n",
    "        self.config.experiment_path.mkdir(parents=True,exist_ok=True)\n",
    "        train_dataloader,val_dataloader,tokenizer_src,tokenizer_tgt=DataTransformationPipeline.main()\n",
    "        model = self.get_model( tokenizer_src.get_vocab_size(), tokenizer_tgt.get_vocab_size()).to(device)\n",
    "        optimizer = torch.optim.Adam(model.parameters(), lr=self.config.lr, eps=1e-9)\n",
    "        initial_epoch = 0\n",
    "        global_step = 0\n",
    "    # Check if there is a pre-trained model to load\n",
    "        if self.config.preload:\n",
    "            model_filename = self.get_weights_file_path(self.config.preload)\n",
    "            logger.info(f\"preloading model {model_filename}\")\n",
    "            print(f'Preloading model {model_filename}')\n",
    "            state = torch.load(model_filename)\n",
    "            \n",
    "            # Sets epoch to the saved in the state plus one, to resume from where it stopped\n",
    "            initial_epoch = state['epoch'] + 1\n",
    "            writer = SummaryWriter(self.config.experiment_path)\n",
    "            optimizer.load_state_dict(state['optimizer_state_dict'])\n",
    "            global_step = state['global_step']\n",
    "        loss_fn = nn.CrossEntropyLoss(ignore_index=tokenizer_src.token_to_id('[PAD]'), label_smoothing=0.1).to(device)\n",
    "        previous_model_filename = None  # Variable to track the last saved model file\n",
    "        for epoch in range(initial_epoch, self.num_epochs):\n",
    "            batch_iterator = tqdm(train_dataloader, desc=f'Processing epoch {epoch:02d}')\n",
    "            for i, batch in enumerate(batch_iterator):\n",
    "                model.train()\n",
    "                \n",
    "                # Loading input data and masks onto the GPU\n",
    "                encoder_input = batch['encoder_input'].to(device)\n",
    "                decoder_input = batch['decoder_input'].to(device)\n",
    "                encoder_mask = batch['encoder_mask'].to(device)\n",
    "                decoder_mask = batch['decoder_mask'].to(device)\n",
    "                \n",
    "                # Running tensors through the transformer\n",
    "                encoder_output = model.encode(encoder_input, encoder_mask)\n",
    "                decoder_output = model.decode(encoder_output, encoder_mask, decoder_input, decoder_mask)\n",
    "                proj_output = model.project(decoder_output)\n",
    "                # Loading the target labels onto the GPU\n",
    "                label = batch['label'].to(device)\n",
    "                # Computing loss between model's output and true labels\n",
    "                loss = loss_fn(proj_output.view(-1, tokenizer_tgt.get_vocab_size()), label.view(-1))\n",
    "                # Updating progress bar\n",
    "                batch_iterator.set_postfix({f'loss': f'{loss.item():6.3f}'})\n",
    "                writer.add_scalar('train loss', loss.item(), global_step)\n",
    "                writer.flush()\n",
    "                # Performing backpropagation\n",
    "                loss.backward()\n",
    "                optimizer.step()\n",
    "                # Clearing the gradients to prepare for the next batch\n",
    "                optimizer.zero_grad()\n",
    "                global_step += 1  # Updating global step coun\n",
    "                if global_step % 100 == 0:\n",
    "                    print(f'Iteration {global_step}: loss = {loss.item():6.3f}')\n",
    "                    writer.add_scalar('iteration loss', loss.item(), global_step)\n",
    "                    writer.flush()\n",
    "            run_validation(model, val_dataloader, tokenizer_src, tokenizer_tgt, config['seq_len'], device, lambda msg: batch_iterator.write(msg), global_step, writer)\n",
    "        \n",
    "            # Save model after every epoch\n",
    "            model_filename = get_weights_file_path(config, f'epoch_{epoch+1}')\n",
    "            \n",
    "            # Writing current model state to the model_filename\n",
    "            torch.save({\n",
    "                'epoch': epoch,  # Current epoch\n",
    "                'model_state_dict': model.state_dict(),  # Current model state\n",
    "                'optimizer_state_dict': optimizer.state_dict(),  # Current optimizer state\n",
    "                'global_step': global_step  # Current global step\n",
    "            }, model_filename)\n",
    "            \n",
    "            print(f'Saved model for epoch {epoch+1}: {model_filename}')\n",
    "            \n",
    "            # Delete the model from the previous epoch\n",
    "            if previous_model_filename and os.path.exists(previous_model_filename):\n",
    "                os.remove(previous_model_filename)\n",
    "                print(f'Deleted previous model: {previous_model_filename}')\n",
    "            \n",
    "            # Update the previous model filename to the current one\n",
    "            previous_model_filename = model_filename\n",
    "\n",
    "        print(\"\\nTraining complete. Calculating BLEU score on validation data...\")\n",
    "        avg_bleu_score = calculate_bleu_for_validation(model, val_dataloader, tokenizer_src, tokenizer_tgt, config['seq_len'], device)\n",
    "        print(f\"\\nFinal Average BLEU score on validation data: {avg_bleu_score:.4f}\")\n",
    "                \n",
    "        \n",
    "\n",
    "\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "ename": "ImportError",
     "evalue": "cannot import name 'casual_mask' from 'translator.utils.common' (/media/puzan/NewVolume/MyFolder/src/translator/utils/common.py)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mImportError\u001b[0m                               Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[30], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtranslator\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mutils\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mcommon\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m casual_mask\n",
      "\u001b[0;31mImportError\u001b[0m: cannot import name 'casual_mask' from 'translator.utils.common' (/media/puzan/NewVolume/MyFolder/src/translator/utils/common.py)"
     ]
    }
   ],
   "source": [
    "from translator.utils.common import casual_mask"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Inference:\n",
    "    def __init__(self,config:ModelTrainingConfig) :\n",
    "        self.config=config\n",
    "    \n",
    "    def greedy_decode(self,model,source,source_mask,tokenizer_src,tokenizer_tgt,device):\n",
    "        sos_idx=tokenizer_tgt.token_to_id('[SOS]')\n",
    "        eos_idx=tokenizer_tgt.token_to_id('[EOS]')\n",
    "\n",
    "    # computing the output of the encoder \n",
    "        encoder_output=model.encode(source,source_mask)\n",
    "        decoder_input=torch.empty(1,1).fill_(sos_idx).type_as(source).to(device) #tensor type is like source\n",
    "        while True:\n",
    "            if decoder_input.size(1)==self.config.max_seq_len:\n",
    "                break\n",
    "            #building a mask for decoder input \n",
    "            decoder_mask=casual_mask(decoder_input.size(1)).type_as(source_mask).to(device)\n",
    "            #calculating the output of the decoder\n",
    "            out=model.decode(encoder_output,source_mask,decoder_input,decoder_mask)\n",
    "            prob=model.project(out[:,-1])\n",
    "\n",
    "            # Select token with the highest probability \n",
    "            _,next_word=torch.max(prob,dim=1)\n",
    "            decoder_input=torch.cat([decoder_input,torch.empty(1,1).type_as(source).fill_(next_word.item()).to(device)],dim=1)\n",
    "            if next_word==eos_idx:\n",
    "                break\n",
    "    def run_validation(self,model,validation_ds,tokenizer_src,tokenizer_tgt,device,print_msg, global_state, writer, num_examples=2):\n",
    "        model.eval()\n",
    "        count=0\n",
    "        console_width=0\n",
    "        #evaluation loop\n",
    "        with torch.no_grad():\n",
    "            for batch in validation_ds:\n",
    "                count+=1\n",
    "                encoder_input=batch['encoder_input'].to(device)\n",
    "                encoder_mask=batch['encoder_mask'].to(device)\n",
    "                assert encoder_input.size(0)==1, 'Batch size must be 1 for validation.'\n",
    "                model_out=greedy_decode(model,encoder_input,encoder_mask,tokenizer_src,tokenizer_tgt,device)\n",
    "\n",
    "\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#greedy decode for inferenceing\n",
    "\n",
    "def greedy_decode(model,source,source_mask,tokenizer_src,tokenizer_tgt,max_len,device):\n",
    "    #retrieving the indices from the start and end of sequences\n",
    "    sos_idx=tokenizer_tgt.token_to_id('[SOS]')\n",
    "    eos_idx=tokenizer_tgt.token_to_id('[EOS]')\n",
    "\n",
    "    # computing the output of the encoder \n",
    "    encoder_output=model.encode(source,source_mask)\n",
    "    decoder_input=torch.empty(1,1).fill_(sos_idx).type_as(source).to(device) #tensor type is like source\n",
    "    while True:\n",
    "        if decoder_input.size(1)==max_len:\n",
    "            break\n",
    "        #building a mask for decoder input \n",
    "        decoder_mask=casual_mask(decoder_input.size(1)).type_as(source_mask).to(device)\n",
    "        #calculating the output of the decoder\n",
    "        out=model.decode(encoder_output,source_mask,decoder_input,decoder_mask)\n",
    "        prob=model.project(out[:,-1])\n",
    "\n",
    "        # Select token with the highest probability \n",
    "        _,next_word=torch.max(prob,dim=1)\n",
    "        decoder_input=torch.cat([decoder_input,torch.empty(1,1).type_as(source).fill_(next_word.item()).to(device)],dim=1)\n",
    "        if next_word==eos_idx:\n",
    "            break\n",
    "\n",
    "    # sequence of tokens generated by the decoder\n",
    "    return decoder_input.squeeze(0)\n",
    "def run_validation(model, validation_ds, tokenizer_src, tokenizer_tgt, max_len, device, print_msg, global_state, writer, num_examples=2):\n",
    "    model.eval()\n",
    "    count=0 # initializing counter to keep track of how many examples have been processed\n",
    "    \n",
    "    console_width=80 # fixed width for printed messages\n",
    "    \n",
    "    # creating evaluation loop\n",
    "    with torch.no_grad(): # ensuring that no gradients are computed during this process\n",
    "        for batch in validation_ds:\n",
    "            count+=1\n",
    "            encoder_input=batch['encoder_input'].to(device)\n",
    "            encoder_mask=batch['encoder_mask'].to(device)\n",
    "            \n",
    "            # ensuring that the batch_size of the validation set is 1\n",
    "            assert encoder_input.size(0)==1, 'Batch size must be 1 for validation.'\n",
    "            \n",
    "            # applying the `greedy_decode` functio to get the model's output of the source text of the input batch\n",
    "            model_out=greedy_decode(model, encoder_input, encoder_mask, tokenizer_src, tokenizer_tgt, max_len, device)\n",
    "            \n",
    "            # retraeving source and target texts from the batch\n",
    "            source_text=batch['src_text'][0]\n",
    "            target_text=batch['tgt_text'][0] # true translation\n",
    "            # token_ids = model_out.argmax(dim=-1).squeeze().tolist() # Convert tensor to a list of token IDs\n",
    "            # model_out_text = tokenizer_tgt.decode(token_ids) \n",
    "            model_out_text=tokenizer_tgt.decode(model_out.detach().cpu().numpy()) # decoded, human-readable model ouptut\n",
    "            \n",
    "            # printing results\n",
    "            print_msg('-'*console_width)\n",
    "            print_msg(f'SOURCE: {source_text}')\n",
    "            print_msg(f'TARGET: {target_text}')\n",
    "            print_msg(f'PREDICTED: {model_out_text}')\n",
    "            \n",
    "            # After two examples, we break the loop\n",
    "            if count >= num_examples:\n",
    "                break"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
